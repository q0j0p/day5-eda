{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science workflow\n",
    "\n",
    "Whenever you start a data science project, you should follow a workflow, which will help you:\n",
    "\n",
    "* Perform all steps in analysis\n",
    "* Produce reproducible results and track data provenance\n",
    "* Avoid simple errors\n",
    "* Produce higher quality work\n",
    "\n",
    "The [Common industry standard process for data mining](https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining) (CRISP-DM) is a good workflow to use, unless you know better.  \n",
    "\n",
    "Make sure that you also think about correctness, i.e., verifying that your code correctly implements your model ('solves the equations right') and validating that your model has high fidelity with reality ('solves the right equations').  See [Verification and Validation in Scientific Computing](http://www.amazon.com/Verification-Validation-Scientific-Computing-Oberkampf/dp/0521113601/ref=sr_1_1?ie=UTF8&qid=1445036147&sr=8-1&keywords=verification+and+validation)for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business understanding\n",
    "\n",
    "Every thing starts with business understanding.  Speak with your stakeholders:\n",
    "\n",
    "* What is the business problem you need to answer?\n",
    "* What are the requirements?\n",
    "* How do you measure success?\n",
    "\n",
    "Do not proceed until you have answered these questions.  Often, it is not clear what success looks like or even what you should use as a lable (target) to train you model.  The metric for success will typically be a business quantity like 'decrease churn rate 10%' instead of improving AUC or MAPE.  Consequently, you need to tune your model based on the right business outcome.  Make sure you always state results in business terms like this policy will save $100 MM or decrease fraud by 10%.\n",
    "\n",
    "Note: These steps are an interative process. E.g., after performing a step, such as **Modeling**, you may discover a mistake which causes you to repeat an earlier process, such as **data cleaning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data understanding\n",
    "\n",
    "After you define the business problem, you need to determine what data is available.  Ponder the following:\n",
    "\n",
    "* What datasets are available?\n",
    "* How can you combine them to produce a dataset to answer their business questions?\n",
    "* Do you need to collect additional data?\n",
    "* Does your data have a label (target) or do you need to generate one, perhaps by using [MechTurk](https://www.mturk.com/mturk/welcome)or equivalent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    "To prepare a dataset for modeling, you should first explore the data and, concurrently, figure out how to clean it.  At the end of this step, you should have a dataset you can use to build a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Data cleaning\n",
    "\n",
    "Start by loading your data so that you can begin exploring it.  Perform only the most minimal cleaning necessary -- overcleaning can remove valuable information (signal).  Pro tip:  if your data is huge, start by making sure everything works on a small subset of your data, like a single shard.  You want to be able to interate quickly and get your pipeline working before attempting full-scale analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Exploratory data analysis (EDA)\n",
    "\n",
    "Get to know the strengths and weaknesses of your data:\n",
    "\n",
    "* What are the strengths and weaknesses?\n",
    "* Any weird values?  outliers? missing values? malformed/unstructured fields?\n",
    "* What is the nature of your missing values?  Are they missing at random?  If not, how are you going to deal with them?\n",
    "* Compute summary statistics\n",
    "* Plot features to see if they have predictive power?  If you have a lot of data, draw a subset -- and make sure your results don't depend on the subset you have chosen.\n",
    "* Plot histograms of label and key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Feature engineering\n",
    "\n",
    "Finally, assemble your final dataset.  Feature engineering -- how you construct the features for your model -- is often more important than what model you choose.  Some issues:\n",
    "\n",
    "* Handling missing values -- can you bin or are missing values *missing at random* so you can drop them?\n",
    "* Handling outliers -- should you bin the data to make it discrete?\n",
    "* Replacing categorical variables with dummy variables\n",
    "* Transform data, e.g., take `log` of data, which is often useful with long-tailed data\n",
    "* Convert text data to features using *Natural Language Processing* (NLP), *term frequency-inverse document frequency* (TF-IDF), [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing) trick, n-grams, etc.\n",
    "* Rationalize address data into standard USPS format\n",
    "\n",
    "Now you should be ready to start modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##6. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
